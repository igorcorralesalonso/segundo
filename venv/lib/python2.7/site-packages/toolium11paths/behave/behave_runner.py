# -*- coding: utf-8 -*-

# Copyright (c) Telef√≥nica Digital.
# CDO QA Team <qacdo@telefonica.com>

import argparse
import ast
import json
import os
import re
import sys
from itertools import product
from subprocess import Popen

from toolium11paths.testlink.testlink_utils import get_test_case_ids_in_test_plan_by_name


def get_behave_executions(runner_config_file, behave_user_data={}):
    """
    Construct a list of executions with properties defined in file

    :param runner_config_file: filename with properties list
    :param behave_user_data: dict with behave user data parameters
    :return: list of test executions
    """
    with open(runner_config_file) as json_file:
        runner_properties = json.load(json_file)
    return get_behave_executions_from_properties(runner_properties, behave_user_data)


def add_prop_value(execution_command, prop_name, prop_value):
    """
    Append a formatted command line property to the execution command

    :param execution_command: list with previous execution command properties
    :param prop_name: behave property name
    :param prop_value: behave property value
    """
    # Add property prefix
    prefix = {'behave_scenarios': '-n', 'behave_tags': '-t'}
    if prop_name in prefix:
        execution_command.append(prefix[prop_name])
    # Add property value
    behave_prop_names = ['behave_features', 'behave_scenarios', 'behave_tags', 'behave_args']
    if prop_name in behave_prop_names:
        execution_command.append(prop_value)
    else:
        execution_command.append('-D {}={}'.format(prop_name, prop_value))


# TODO: allow exceptions in combinations
def get_behave_executions_from_properties(runner_properties, behave_user_data={}):
    """
    Construct a list of executions with given properties. If a property value is a list, there will be an execution for
    each property value in list.

    :param runner_properties: list of dicts with the properties that should be used to run tests
    :param behave_user_data: dict with behave user data parameters
    :return: list of test executions
    """
    executions = []
    for runner_property in runner_properties:
        execution = []
        combinations = []
        for prop_name, prop_value in list(runner_property.items()):
            if prop_name not in behave_user_data.keys():
                if isinstance(prop_value, list):
                    if prop_name in ['behave_scenarios', 'behave_tags', 'behave_args']:
                        for single_value in prop_value:
                            add_prop_value(execution, prop_name, single_value)
                    else:
                        prop_name_combinations = []
                        for single_value in prop_value:
                            add_prop_value(prop_name_combinations, prop_name, single_value)
                        combinations.append(prop_name_combinations)
                else:
                    add_prop_value(execution, prop_name, prop_value)

        for prop_name, prop_value in behave_user_data.items():
            if prop_value[:1] in ['[', '{'] and prop_value[-1:] in [']', '}']:
                prop_value_list = ast.literal_eval(prop_value)
                for prop_value_item in prop_value_list:
                    add_prop_value(execution, prop_name, prop_value_item)
            else:
                add_prop_value(execution, prop_name, prop_value)

        if not combinations:
            executions.append(execution)
        else:
            for combination in product(*combinations):
                executions.append(execution + list(combination))
    return executions


def update_junit_files(execution_properties, execution_index, has_epic=False):
    """
    Reads all junit output files and adds the execution label to the files names and to the test suites names to
    differentiate between executions

    :param execution_properties: list with behave execution properties
    :param execution_index: execution index
    :param has_epic: if True, add as part of the junit output filename the name of the epic of the feature
    """
    # Get junit folder from properties
    junit_folder = get_junit_folder(execution_properties)
    if junit_folder is None:
        # junit output is not configured
        return

    # Get execution label
    execution_label = '%s_%s' % (execution_index, get_execution_label(execution_properties))

    # Get epic_name
    epic_name = get_epic(execution_properties) if has_epic else None

    if not os.path.exists(junit_folder):
        # junit output has not been created, a driver initialization error may have occurred
        os.makedirs(junit_folder)
        junit_files = []
    else:
        # Get all files from junit folder
        junit_files = [os.path.join(junit_folder, f) for f in os.listdir(junit_folder) if f.startswith('TESTS-')]

    if not junit_files:
        # If there are no junit files, a driver init error or a connection error may have occurred
        junit_files = [create_global_error_xml(junit_folder)]
    for junit_file in junit_files:
        update_junit_file(junit_file, execution_label, epic_name)

    # Remove temporal subfolder
    os.rmdir(junit_folder)


def create_global_error_xml(junit_folder):
    """
    Create a default failed xml to force a Jenkins job error

    :param junit_folder: folder to save the junit report
    :returns: xml path
    """
    failed_xml = """<?xml version='1.0' encoding='UTF-8'?>
        <testsuite errors="0" failures="1" name="global.Global error" skipped="0" tests="1" time="0.0">
        <testcase classname="global.Global error" name="Global error" status="failed" time="0.0">
        <failure type="UnknownError"></failure>
        </testcase>
        </testsuite>
    """
    global_error_xml = os.path.join(junit_folder, 'TESTS-global.xml')
    with open(global_error_xml, 'w') as junit_file:
        junit_file.write(failed_xml)
    return global_error_xml


def update_junit_file(junit_file, execution_label, epic_name=None):
    """
    Adds the execution label to the file name and to the test suite name to differentiate between executions

    :param junit_file: absolute path to junit xml file
    :param execution_label: label that identifies the execution
    :param epic_name: if given, add as part of the suite name, the name of the EPIC
    """
    # Open junit file
    with open(junit_file, 'r') as f:
        file_content = f.read()

    # Add execution label to test suite name
    match = re.compile('TESTS-(.*).xml').search(junit_file)
    if match:
        suite_name = match.group(1)
        base_suite_name = '{}.{}'.format(epic_name, suite_name) if epic_name else suite_name
        file_content = file_content.replace('name="{}'.format(suite_name),
                                            'name="{}_{}'.format(execution_label, base_suite_name))

    # Write modified content to a new junit file in parent folder
    base_file_name = '{}-{}'.format(execution_label, epic_name) if epic_name else execution_label
    new_junit_file = junit_file.replace('TESTS-', '{}-'.format(base_file_name))
    new_junit_file = os.path.join(os.path.dirname(os.path.dirname(new_junit_file)), os.path.basename(new_junit_file))

    with open(new_junit_file, 'w') as f:
        f.write(file_content)

    # Remove old junit file
    os.remove(junit_file)


def get_execution_label(execution_properties):
    """
    Create a execution label from execution properties (Driver_type and TestExecution_language)

    :param execution_properties: list with behave execution properties
    :returns: execution label
    """
    driver_type = ''
    language = ''
    for prop in execution_properties:
        match = re.compile('-D Driver_type=(.*)').search(prop)
        if match:
            driver_type = match.group(1)
        match = re.compile('-D TestExecution_language=(.*)').search(prop)
        if match:
            language = match.group(1)
    if driver_type and language:
        execution_label = '{}-{}'.format(driver_type, language)
    elif not language:
        execution_label = driver_type
    elif not driver_type:
        execution_label = language
    return execution_label


def get_junit_folder(execution_properties):
    """
    Get junit folder from execution properties

    :param execution_properties: list with behave execution properties
    :returns: junit folder
    """
    for idx, prop in enumerate(execution_properties):
        if prop == '--junit-directory':
            try:
                # Next property should contain the junit folder
                return execution_properties[idx + 1]
            except IndexError:
                return None
    return None


def add_subfolder_to_junit_folder(execution_properties, subfolder):
    """
    Get junit folder from execution properties and update it with a temporal subfolder

    :param execution_properties: list with behave execution properties
    :param subfolder: subfolder name that will be added to the junit folder
    :returns: original junit folder
    """
    for idx, prop in enumerate(execution_properties):
        if prop == '--junit-directory':
            try:
                # Next property should contain the junit folder
                orig_junit_folder = execution_properties[idx + 1]
                # Add a temporal subfolder
                execution_properties[idx + 1] = os.path.join(orig_junit_folder, subfolder)
                return orig_junit_folder
            except IndexError:
                return None
    return None


def valid_user_data(param):
    """
    Validate user data parameters

    :param param: user data parameter
    :returns: dict with parameter name and value
    """
    try:
        splited_param = param.split('=')
        return {splited_param[0]: splited_param[1]}
    except IndexError:
        raise argparse.ArgumentTypeError("expected a value after the parameter name: '%s'." % param)


def get_epic(execution_properties):
    """
    Get the epic of a feature. This mean, the name of the last directory containing the features to run.
     Features should be located under "features" directory in the test project.

    For instance:

        features/ -> EPIC: None
        features/my_feature_01.feature -> EPIC: None
        features/epic01/ -> EPIC: epic01
        features/epic01/my_feature_01.feature -> EPIC: epic01.1
        features/epic01/epic01.1 -> EPIC: epic01.1
        features/epic01/epic01.1/my_feature_01.feature -> EPIC: epic01.1

    :param execution_properties: list with behave execution properties
    :return: The name of the ECPIC or None if it does not apply.
    """

    feature = None
    for exec_prop in execution_properties:
        if "features" in exec_prop:
            feature = exec_prop
            break

    if feature:
        if os.path.normpath(feature).endswith('.feature'):
            epic = os.path.basename(os.path.dirname(feature))
        else:
            epic = os.path.basename(os.path.normpath(feature))

        if epic and epic != 'features':
            return epic

    return None


def behave_runner_cli(args):
    # Convert user data list into a dict
    behave_user_data = {}
    for user_data in args.user_data:
        behave_user_data.update(user_data)

    print('\n-----------------')
    print('- Behave runner -')
    print('-----------------')
    print('Reading runner properties from {}'.format(args.config_file))
    executions = get_behave_executions(args.config_file, behave_user_data)
    print('Running {} test executions'.format(len(executions)))
    sys.stdout.flush()

    parallel_processes = []
    for idx, execution_properties in enumerate(executions):
        # Use a temporal folder instead of configured one
        add_subfolder_to_junit_folder(execution_properties, 'tmp%s' % idx)

        command = ['behave'] + args.features + execution_properties
        if args.testlink_execution:
            # Get all TestCases Ids from TestLink
            #   filtering by the configured TestPlan, Build and Platform in toolium.cfg
            # Run only Scenarios with TL.<ID> include in the retrieved list.
            command += ["-t TL." + ",TL.".join(get_test_case_ids_in_test_plan_by_name())]

        print('\n-----------------')
        print('Test execution {}: {}'.format(idx + 1, ' '.join(command)))
        print('-----------------\n')
        sys.stdout.flush()
        try:
            process = Popen(command)
            if args.parallel:
                parallel_processes.append({'process': process, 'execution_properties': execution_properties,
                                           'index': str(idx + 1), 'has_epic': args.epic_junit})
            else:
                process.wait()
                update_junit_files(execution_properties, str(idx + 1), has_epic=args.epic_junit)
        except Exception as e:
            print('Error executing command: {}'.format(e))
            sys.stdout.flush()
            update_junit_files(execution_properties, str(idx + 1), has_epic=args.epic_junit)

    for process in parallel_processes:
        process['process'].wait()
        update_junit_files(process['execution_properties'], process['index'], process['has_epic'])


# TODO: accept behave arguments in command line
def behave_runner_cli_definition(parser, default_runner_config_file=None):
    """
    CLI definition for Behave Runner
    :param parser to define CLI arguments
    :param default_runner_config_file: file used if -f option is not found
    """

    if default_runner_config_file:
        parser.add_argument('-f', metavar='config_file', dest='config_file', default=default_runner_config_file,
                            help='runner config file')
    else:
        required = parser.add_argument_group('required named arguments')
        required.add_argument('-f', metavar='config_file', dest='config_file', required=True, help='runner config file')
    parser.add_argument('-D', metavar='name=value', dest='user_data', default=[], action='append',
                        help='behave user data parameters', type=valid_user_data)
    parser.add_argument('-p', '--parallel', action='store_true', help='parallel executions')
    parser.add_argument('-e', '--epic-junit', action='store_true', dest='epic_junit', default=False,
                        help='store junit results in features epic directories')
    parser.add_argument('-T', '--testlink-presence', action='store_true', dest='testlink_execution', default=False,
                        help='Run only TestCases included in the configured TestPlan|Build|Environment (toolium.cfg)'
                             'only execute the Scenarios defined into the configured (TestPlan, Build, Platform)')
    parser.add_argument('features', nargs='*', help='behave features')


def run_behave_tests(default_runner_config_file=None):
    """
    Run behave tests multiple times depending on runner properties

    :param parser to define CLI arguments
    :param default_runner_config_file: file used if -f option is not found
    """
    parser = argparse.ArgumentParser()
    behave_runner_cli_definition(parser, default_runner_config_file)

    args = parser.parse_args()
    behave_runner_cli(args)
